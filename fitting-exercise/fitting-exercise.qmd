<<<<<<< HEAD
---
title: "fitting-exercise"
---
## Mavoglurant modeling Exercise (Week 8)

In this exercise, a model is fitted to a data set (mavoglurant_A2121_nmpk) from the R package nlmixr2data First, required packages are installed and loaded.

```{r}
#| message: false
#| warning: false
library(readxl) #for loading Excel files
library(dplyr) #for data processing/cleaning
library(tidyr) #for data processing/cleaning
library(skimr) #for nice visualization of data 
library(here) #to set paths
library(ggplot2) # for plots
library(gtsummary)# for summary tables
library(patchwork) #for combine plots
library(tidymodels)
library(rsample)
library(glmnet)
```

Setting a random seed during analysis step

```{r}
rngseed = 1234
```

Data is loaded

```{r}
# path to data using here function
data_location <- here::here("fitting-exercise","Mavoglurant_A2121_nmpk.csv")
rawdata <- read.csv(data_location)
```

Checking the data

```{r}
head(rawdata)
skimr::skim(rawdata)
```
### Exploratory Data Analysis

Next, three plots of DV over time stratified by the three Dose levels are created to have a visual inspection of the data. These plots are re-created by grouping the observations by ID following Dr. Handel's codes in solution to module 8 Exercise.

```{r}
#exploring data  

p1<- rawdata %>% ggplot()+
      geom_line(aes(x= TIME, y=DV, group = as.factor(ID), color= as.factor(DOSE))) +
      facet_wrap(~DOSE, scales = "free_y")
p1

  
```

The lines are not smooth. As informed in the topic, the reason for this could be that some individuals potentially have received the drug more than once, which is indicated by having both entries with OCC=1 and OCC=2. However, in absence of information on the OCC values, the analysis looks into the data set with OCC=1 only. A subset data frame is created keeping only those observations for which OCC has a value of '1'

```{r}

#Creating a sub set of data including observations with OCC=1
dataclean_step1 <- rawdata %>% 
                    dplyr::filter(OCC==1)
skimr::skim(dataclean_step1)

```

Again, a plot of DV over time stratified by Dose is created.

```{r ehco = TRUE}
#p1.1<- dataclean_step1 %>% ggplot()+
 #     geom_line(aes(x= TIME, y=DV, group = as.factor(ID), color= as.factor(DOSE))) +
  #    facet_wrap(~DOSE, scales = "free_y")
#p1.1

```

Next, two subsets data frame are created. In the first subset, observations wherein TIME=0 are dropped followed by computation of a variable Y which sums up the DV values for each individual based on ID. This resulted into a data frame of size 120 x 2 containing columns for ID and Y. In the second subset, only those observations are kept for which TIME=0. This second sub set has a size of 120 x 17. Finally these two data subsets are joined to create a new data frame of size 120 x 18.

```{r}
#Creating a subset excluding the observations with TIME=0
datasubset_1 <- dataclean_step1 %>% dplyr::filter(TIME != 0) %>%
  #Grouping by ID
   dplyr::group_by(ID) %>%
  #Creating variable Y, which is sum of the DV variables by ID
      dplyr::summarize(Y=sum(DV, na.rm = TRUE), .groups = "drop")

#Creating a subset including only observations with TIME=0 
datasubset_2 <- dataclean_step1 %>% 
                    dplyr::filter(TIME==0)

#Inner joining the two subsets to create a new data frame 
data_joined <- inner_join(datasubset_1, datasubset_2, by = "ID")
#skimr::skim(data_joined)

```

A final data set is created by keeping only the variables Y, DOSE, AGE, SEX, RACE, WT, HT. DOSE, SEX and RACE are coded as factors.

```{r}
#sub set of data_joined keeping only the required variables
data_final <- data_joined %>%
  select(Y, DOSE, AGE, SEX, RACE, WT, HT) %>%
  #converting SEX, RACE and DOSE as factors
  mutate(across (c(SEX, RACE), as.factor))

skimr::skim(data_final)
```

The data summary shows that SEX has 2 levels and RACE has 4 levels. The level 88 of RACE has 8 observations and 7 has 2 observations. In absence of the codebook, it is unclear the reason of naming those levels with numerical values distant from the other two names with top counts. This will be kept in the data for further analyses.

**Exploratory Data Analysis**

A table is created for descriptive analysis by Dose.

```{r}
# Creating a table
table1 <- tbl_summary(
  data_final,
  by = DOSE, # Stratify summary by DOSE
  type = list(
    DOSE ~ "categorical", #Specifying DOSE as categorical
    SEX ~ "categorical",  #Specifying SES as categorical
    RACE ~ "categorical", #Specifying RACE as categorical
    Y ~ "continuous2",
    AGE ~ "continuous2",
    WT ~ "continuous2",
    HT ~ "continuous2"
  ),
  statistic = list(
    all_continuous() ~ c("{mean} ({sd})", "{min}, {max}"), # Statistics for continuous variables
    all_categorical() ~ "{n} ({p}%)"), # Statistics for categorical variables
  missing = "no" # Option to exclude missing data in summary
)

# Display the table
table1

```

A visual inspection of distribution of continuous variables is performed by plotting histograms.

```{r}
#| message: false
# Plotting histograms for all continuous variables in a grid

plot1 <-
  data_final %>%
  select(Y, AGE, WT, HT) %>%
  pivot_longer(everything()) %>%
  mutate(name = factor(name, levels = c("Y", "AGE", "WT", "HT"))) %>%  #Keeps the order of plot
  ggplot(aes(x = value, fill = name)) +
  geom_histogram(alpha = 0.5, color="black") +
  facet_wrap(~name, scales = "free") +
  scale_fill_manual(values = c("Y" = "lightgreen", "AGE" = "dodgerblue", "WT" = "grey", "HT" = "salmon")) +
  theme_minimal()
plot1

#Saving the figure in the folder
plot1_file <- here("fitting-exercise", "hist_plots.png")
ggsave(filename = plot1_file, plot=plot1, bg="white")

```

Bar charts are plotted for visual inspection of distribution of factor variables. The bar chart indicates smaller representation of Dose 37.5, Sex level of 2 and Races 7 and 88 in the data.

```{r}
#creating a combined plot for DOSE, SEX and RACE. For this plot DOSE is treated as a factor as it has only 3 levels.
plot2 <- 
  data_final %>%
  mutate(DOSE = as.factor(DOSE)) %>% # Convert DOSE to factor here
  select(DOSE, SEX, RACE) %>%
  pivot_longer(everything(), names_to = "name", values_to = "value") %>%
  mutate(name = factor(name, levels = c("DOSE", "SEX", "RACE"))) %>%
  ggplot(aes(x = value, fill = name)) +
  geom_bar(alpha = 0.5, color = "black") +
  facet_wrap(~name, scales = "free") +
  scale_fill_manual(values = c("DOSE" = "lightgreen", "SEX" = "salmon", "RACE" = "skyblue"))
plot2

#Saving the figure in the folder
plot2_file <- here("fitting-exercise", "factor_vars_plots.png")
ggsave(filename = plot2_file, plot=plot2, bg="white")

```

Box plot for distribution of Y by Dose is created.The box plot suggests outlier values of Y for doses 25 and 50.

```{r}
plot3 <- data_final %>%
  mutate(DOSE = as.factor(DOSE)) %>% # Converting DOSE to factor only for the plot
  ggplot(aes(x = DOSE, y = Y, fill = DOSE)) + 
  geom_boxplot() + 
  labs(title = "Distribution of Y by DOSE", x = "DOSE", y = "Y") +
  theme_minimal()

plot3

#Saving the figure in the folder
plot3_file <- here("fitting-exercise", "Y_Dose.png")
ggsave(filename = plot3_file, plot=plot3, bg="white")
```

Next, Scatter plot is created for Y by Age stratified by Dose. For doses 37.5 and 50, the plot indicates a linear increase in Y as Age increases. For the dose of 25, such relationship is negative.

```{r}
#| message: false
plot4 <- data_final %>%
  mutate(DOSE = as.factor(DOSE)) %>% # Convert DOSE to factor here
  ggplot(aes(x = AGE, y = Y, group = DOSE, col = DOSE)) +
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + # Add a linear regression line
  scale_color_viridis_d(option = "plasma", end = .7)+
  labs(title = "Scatterplot of Y vs. AGE", x = "Age", y = "Total Drug (Y)") +
  theme_minimal()

plot4

#Saving the figure in the folder
plot4_file <- here("fitting-exercise", "Y_Age_Dose.png")
ggsave(filename = plot4_file, plot=plot4, bg="white")

```

Scatter plot is created for Y by HT stratified by Dose. For all the doses the plot indicates a decline in Y as HT increases.

```{r}
#| message: false
plot5 <- data_final %>%
  mutate(DOSE = as.factor(DOSE)) %>% # Convert DOSE to factor here
  ggplot(aes(x = HT, y = Y, group = DOSE, col = DOSE)) +
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + # Add a linear regression line
  scale_color_viridis_d(option = "plasma", end = .7)+
  labs(title = "Scatterplot of Y vs. HT", x = "HT", y = "Total Drug (Y)") +
  theme_minimal()

plot5

#Saving the figure in the folder
plot5_file <- here("fitting-exercise", "Y_HT_Dose.png")
ggsave(filename = plot5_file, plot=plot5, bg="white")

```

Scatter plot is created for Y by WT stratified by Dose. As with Y and HT, a decline in Y is observed as WT decreased for all levels of Dose.

```{r}
#| message: false
plot6<-data_final %>%
  mutate(DOSE = as.factor(DOSE)) %>% # Convert DOSE to factor here
  ggplot(aes(x = WT, y = Y, group = DOSE, col = DOSE)) +
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + # Add a linear regression line
  scale_color_viridis_d(option = "plasma", end = .7)+
  labs(title = "Scatterplot of Y vs. WT", x = "WT", y = "Total Drug (Y)") +
  theme_minimal()

plot6

#Saving the figure in the folder
plot6_file <- here("fitting-exercise", "Y_WT_Dose.png")
ggsave(filename = plot6_file, plot=plot6, bg="white")

```

Visual inspection of correlation between the continuous variables is performed. The scatterplot matrix indicates a linear correlation between WT and HT.

```{r}
# Using the pairs function for selected variables
plot7 <- pairs(data_final[, c("Y", "AGE", "WT", "HT")], 
      main = "Pairwise Scatterplot Matrix")

plot7

#Saving the figure in the folder
plot7_file <- here("fitting-exercise", "Pair_matrix.png")
ggsave(filename = plot7_file, plot=plot6, bg="white")
```

### Model Fitting

#### Linear Regression Models

A linear model is fitted to the continuous outcome (Y) using the main predictor of interest DOSE with the help of the recommended website https://www.tidymodels.org/start/models/.

```{r}
#using linear regression function from tidymodels to regress Y on DOSE
#The default for linear_reg() is "lm" for OLS
lm_dose <- linear_reg() %>% set_engine("lm") %>% fit(Y ~ DOSE, data = data_final)
#Using tidy for better format of the result table
tidy(lm_dose)

```

The estimates suggests a positive relationship between the Y and DOSE indicating a unit increase in DOSE increases Y by 58.21 units.

RMSE is computed for this model with the help of ChatGPT.

```{r}
#First predict using the model
lm_dose_pred <- predict(lm_dose, new_data = data_final)%>%
  bind_cols(data_final) #adds the predicted values to the same dataframe

lm_dose_pred

#Calculate RMSE
rmse_lm_dose <- lm_dose_pred %>%
  yardstick::rmse(truth = Y, estimate = .pred)

#Calculate R-squared
rsq_lm_dose <- lm_dose_pred %>% 
  yardstick::rsq(truth = Y, estimate = .pred)


metrics_lm_dose <- tibble(
                        Metric = c ("rmse_lm_dose", "rsq_lm_dose"),
                        Value = c(rmse_lm_dose$.estimate, rsq_lm_dose$.estimate))
metrics_lm_dose

```

The RMSE is 666.46 and R-squared is approximately 0.52 for the model containing only dose as the predictor of Y.

Next, a linear model is fitted to the continuous outcome (Y) using the all predictors with the help of the website https://www.tidymodels.org/start/models/ recommended in the class.

```{r}
#using linear regression function from tidymodels to regress Y on DOSE
#The default for linear_reg() is "lm" for OLS
lm_all <- linear_reg() %>% set_engine("lm") %>% fit(Y ~ ., data = data_final)

#Using tidy for better format of the result table
tidy(lm_all)

```

The estimates suggests, controlling for the effects of other variables, there is a positive relationship of Y with each of DOSE and AGE and a negative relation with each of WT and HT. Holding all other variables constant, Y is expected to decrease for a change in SEX variable from 1 to 2 and Y is expected to increase for a change in RACE variable from 1 to 2.

RMSE is computed for this model with the help of ChatGPT.

```{r}
#First predict using the model
lm_all_pred <- predict(lm_all, new_data = data_final)%>%
  bind_cols(data_final) #adds the predicted values to the same dataframe

lm_all_pred

#Calculate RMSE
rmse_lm_all <- lm_all_pred %>%
  yardstick::rmse(truth = Y, estimate = .pred)

#Calculate R-squared
rsq_lm_all <- lm_all_pred %>% 
  yardstick::rsq(truth = Y, estimate = .pred)


metrics_lm_all <- tibble(
                        Metric = c ("rmse_lm_all", "rsq_lm_all"),
                        Value = c(rmse_lm_all$.estimate, rsq_lm_all$.estimate))
metrics_lm_all

```

RMSE measures the average difference between the predicted values from the model and the actual values in the data. A lower RMSE value indicates a better fit to the data. Contrarily, a higher R-squared value is indicative of a model explaining a greater proportion of variance in the dependent variable. Higher R-squared values indicate a model's stronger explanatory power. Comparing the two linear models, the second model has a lower RMSE value of 590.85 compared to 666 of the first model. The R-squared is higher at 0.62 for the second model compared to 0.52 of the first model. This indicates that the linear model containing all predictor variables outperforms the one containing only DOSE in both metrics.

The interpretation should be made with caution and should be contextualized within the analysis domain, the specific scales of the variables involved, and the potential for multicollinearity among predictors. For example, the scatter plot pairwise matrix suggests a high correlation between HT and WT indicating redundancy in the predictors. Adding more predictors can artificially inflate the R-square without necessarily improving the model's predictiveness. As R-squared does not penalize for the inclusion of correlated predictors, it can lead to overestimating the model's explanatory power.

#### Logistic Models

Next, logistic model is fitted for the categorical variable SEX with Dose as the predictor. ROC-AUC and Accuracy are also computed for this model.

```{r}

# Prepare the data: Split into training and testing sets
set.seed(123) #for reproducibility
data_split <- initial_split(data_final, prop = 0.80) #80% of the data set as training data
train_data <- training(data_split)
test_data <- testing(data_split)

# Defining the logistic regression model specification
logistic_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

# Fitting the model to the training data
logistic_fit_dose <- logistic_spec %>%
  fit(SEX ~ DOSE, data = train_data)
tidy(logistic_fit_dose)

# Predicting on the test data
predictions_dose <- predict(logistic_fit_dose, new_data = test_data, type = "prob")

# Adding the predicted probabilities back to the test set for evaluation
test_data <- bind_cols(test_data, predictions_dose)
test_data

# Calculating ROC AUC and Accuracy
# SEX is a factor with levels "1" and "2", and the interest is in the level "1" predictions
roc_auc_dose <- roc_auc(test_data, truth = SEX, .pred_1)

test_data <- test_data %>%
  mutate(predicted_class = if_else(.pred_1 > 0.5, '1', '2'),
         predicted_class = factor(predicted_class, levels = levels(SEX)))

# Now calculate accuracy
accuracy_dose <- accuracy(test_data, truth = SEX, estimate = predicted_class)

# Create a tibble to hold the metrics
metrics_table <- tibble(
  Metric = c("ROC AUC", "Accuracy"),
  Value = c(roc_auc_dose$.estimate, accuracy_dose$.estimate) # Extract the metric values
)
metrics_table
```

With the assumption that R treats the first level of a factor ('1' in this case) as the reference category in logistic regression, the above estimate suggests a negative association of DOSE with the probability of SEX being 2.

The capacity of this model to distinguish between SEX levels based on DOSE is 0.5 which indicates an ability same as random guessing. The accuracy is 0.92, which suggests this model has predicted 92% of the SEX correctly.

Next, logistic model is fitted for the categorical variable SEX using all the predictors.

```{r}
#| warning: false
# Preparing the data by splitting into training and testing sets
set.seed(123) #for reproducibility
data_split <- initial_split(data_final, prop = 0.80)
train_data <- training(data_split)
test_data <- testing(data_split)

# Defining the logistic regression model specification
logistic_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

# Fitting the model to the training data
logistic_fit_all <- logistic_spec %>%
  fit(SEX ~ Y + DOSE + AGE + RACE + WT + HT, data = train_data)

tidy(logistic_fit_all)

# Predicting on the test data
predictions <- predict(logistic_fit_all, new_data = test_data, type = "prob")


# Adding the predicted probabilities back to the test set for evaluation
test_data <- bind_cols(test_data, predictions)


# Calculating ROC AUC and Accuracy
# SEX is a factor with levels "1" and "2", and the interest is in the level "1" predictions
roc_auc_all <- roc_auc(test_data, truth = SEX, .pred_1)

test_data <- test_data %>%
  mutate(predicted_class = if_else(.pred_1 > 0.5, '1', '2'),
         predicted_class = factor(predicted_class, levels = levels(SEX)))

# Calculating accuracy
accuracy_all <- accuracy(test_data, truth = SEX, estimate = predicted_class)

# Creating a tibble to hold the metrics
metrics_table <- tibble(
  Metric = c("ROC AUC", "Accuracy"),
  Value = c(roc_auc_all$.estimate, accuracy_all$.estimate) # Extract the metric values
)
metrics_table

```

With the assumption that R treats the first level of a factor ('1' in this case) as the reference category in logistic regression, the above estimate suggests each of the numerical predictors, keeping other variables constant, has a negative association with the probability of SEX being 2 except for AGE. Compared to RACE1, the probability of SEX being 2 decrease for RACE2.

The second model has a higher ROC-AUC (0.95) compared to the first model (0.50) suggesting that the second model is comparatively better in distinguishing SEX levels. On the other hand, this model has a slightly lower accuracy (0.875) compared to the first model (0.917). Considering both these metrics, the second model containing all predictor variables is a better choice.

## Fitting-exercise continuation (Week 10)

This is extension of exercise 8 using advanced linear modelling techniques.

A final data set is created by keeping only the variables Y, DOSE, AGE, SEX, WT and HT. SEX is coded as factor. RACE is not included in the data. The original summary shows that RACE has 4 levels. The level 88 of RACE has 8 observations and 7 has 2 observations. In absence of the codebook, it is unclear the reason of naming those levels with numerical values distant from the other two names with top counts.

```{r}
#Dropping Race from the data

data_final_new <- subset(data_final, select = -RACE)

```

The data is randomly splitted into 75% train and 25% test sets following the [Data Splitting sectoin of the get Started tidymodels tutorial](https://www.tidymodels.org/start/recipes/#data-split).Linear model is fitted on the training data. In later part of the exercise the model performance will be measured by applying it on the testing data.

```{r}
#setting the random seed for reproducibility
set.seed(rngseed)

#Assigning 75% of the data into the training set
data_split <- initial_split(data_final_new, prop = .75)

#Creating data frames for the train and test data
train_data <- training(data_split)
test_data <- testing(data_split)

#checking the data structure
str(train_data)
str(test_data)

```

### Model performance assessment 1

Two linear models are fitted to the continuous outcome (Y) of the train data, first using only the main predictor of interest DOSE and second using all predictors. A null model is also fitted.

```{r}
#For reproducibility
set.seed(rngseed)

#Using linear regression function from tidymodels. The default for linear_reg() is "lm" for OLS
lin_mod <- linear_reg() %>% set_engine("lm") 

#fitting Y on Dose
linfit_dose1 <- lin_mod%>%fit(Y ~ DOSE, data = train_data)

#Fitting Y on all other variables in the data
linfit_all1 <- lin_mod%>%fit(Y~., data=train_data)

# Fitting a null model using tidymodels' parsnip engine
null_mod <- null_model() %>% set_engine("parsnip") %>% set_mode("regression")
linfit_null1 <- null_mod %>%
  fit(Y ~ 1, data = train_data)

#Using tidy for better format of the result table
tidy(linfit_dose1)
tidy(linfit_all1)
tidy(linfit_null1)

```

The results of both the first and second models suggest a positive relationship between the Y and DOSE. The second model suggests that controlling for the effects of other variables, there is a positive relationship of Y with AGE and a negative relation with each of WT and HT. Holding all other variables constant, Y is expected to decrease for a change in SEX variable from 1 to 2.

Next, RMSE metrics are computed for all the models following Dr. Handel's codes for solutions to module 8 exercise.

```{r}
#| message: false
#| warning: false
#Computing the RMSE for model 1
metrics_dose1 <- linfit_dose1 %>%
  predict(train_data) %>%
  bind_cols(train_data)%>%
  metrics(truth=Y, estimate=.pred)

#Computing the RMSE for model 2
metrics_all1 <- linfit_all1 %>%
  predict(train_data) %>%
  bind_cols(train_data)%>%
  metrics(truth=Y, estimate=.pred)

#Computing the RMSE for model 3
metrics_null1 <- linfit_null1 %>%
  predict(train_data) %>%
  bind_cols(train_data)%>%
  metrics(truth=Y, estimate=.pred)


#print the results
print(metrics_null1)
print(metrics_dose1)
print(metrics_all1)

```

The RMSEs are 948, 702 and 627 for the null model, model with only DOSE and model with all the predictors respectively. RMSE measures the average difference between the predicted values from the model and the actual values in the data. A lower RMSE value indicates a better fit to the data. Thus, the linear model containing all predictor variables outperforms the other two models according to the RMSE metrics.

### Model performance assessment 2

The model performance is computed using cross-validation technique with 10-folds for the two models. This CV technique subsamples the training data 10 times and fits each of the models to the data, wherein 90% of the data is used to fit the model, and 10% to evaluate the model.

```{r}
#setting the random seed for reproducibility
set.seed(rngseed)

folds <-vfold_cv(train_data, v=10)
folds
```

In the following steps, an object is built for resampling using 'workflow' function of tidymodels. The 'workflow' function bundles together the pre-processing, modeling, and post-processing requests.

```{r}
#setting the random seed for reproducibility
set.seed(rngseed)
#Resampling using workflow for the model with only DOSE as predictor
linfit_dose2 <- 
	workflow() %>%
	add_model(lin_mod) %>%
  add_formula(Y ~ DOSE)%>%
	fit_resamples(folds)

#Resampling using workflowfor for the model with all predictors
linfit_all2 <- 
	workflow() %>%
	add_model(lin_mod) %>%
  add_formula(Y ~ .)%>%
	fit_resamples(folds)

#extracting the performance statistics results created from the 10 assessment sets. 

collect_metrics(linfit_dose2)
collect_metrics(linfit_all2)

```

The application of 10-fold CV led to an improvement in the RMSE of the model including only DOSE, reducing it to 690 from 702, which was observed when the training data was utilized without CV. Conversely, the RMSE of the model including all the predictors experienced an increase to 645 with 10-fold CV, compared to 627 without CV.

Unlike the train/test model without CV, the 10-fold CV calculates 10 separate RMSE values for each sample and averages these values. This process introduces some variability among the generated RMSEs. Analysis revealed that the full model exhibits a smaller standard error of 64.81 for RMSE compared to 67.49 of the model that includes only the DOSE predictor.

Finally, the 10-fold CV modelling is performed again to check how different the metric is when using a different seed for randomization.

```{r}
#setting a different random seed
set.seed(222)

#Assigning 75% of the data into the training set
data_split_new <- initial_split(data_final, prop = .75)

#Creating data frames for the train and test data
train_data_new <- training(data_split_new)
test_data_new <- testing(data_split_new)

```

Preparing the data for 10-fold cross-validation.

```{r}
#| warning: false

#setting a different random seed
set.seed(222)

#Creating 10 random samples of the newly generated training data
folds_new <-vfold_cv(train_data_new, v=10)

```

Using the workflow to compute both the models.

```{r}
#setting the random seed for reproducibility
set.seed(222)

#Resampling using workflow for the model with only DOSE as predictor
linfit_dose2_new <- 
	workflow() %>%
	add_model(lin_mod) %>%
  add_formula(Y ~ DOSE)%>%
	fit_resamples(folds_new)

#Resampling using workflowfor for the model with all predictors
linfit_all2_new <- 
	workflow() %>%
	add_model(lin_mod) %>%
  add_formula(Y ~ .)%>%
	fit_resamples(folds_new)

#extracting the performance statistics results created from the 10 assessment sets. 

collect_metrics(linfit_dose2_new)
collect_metrics(linfit_all2_new)

```

When a new seed is applied, further reduction is noted in the mean RMSE for the model that included only DOSE as predictor, with value dropping to 665 from the previous seed result of 691.The Standard Error remained relatively unchanged from the previous figure. For the full model, the mean RMSE slightly increased to 647 with the new seed from 646 with the the previous seed. In both cases, the RMSE of the full model remained higher than that of the full model using the training data without CV.

In summary, the model that included only DOSE, when subjected to 10-fold CV, showed an optimized RMSE compared to the models fitted on the training data without CV. On the other hand, for the full model, a better RMSE is observed when the model is fitted on the training data without CV.

# This section added by MUTSA NYAMURANGA

## Model Predictions

I create 3 data frames that combine the observed and predicted values from the 3 original model fits to all of the training data. I also create labels for each model. 

```{r}
# Creating a data-frame with observed and predicted values from the model with `DOSE` as the predictor
fitted_dose <- linfit_dose1 %>%
  predict(train_data) %>%
  bind_cols(train_data)

# Creating a data-frame with observed and predicted values from the model with everything as the predictor
fitted_all <- linfit_all1 %>%
  predict(train_data) %>%
  bind_cols(train_data)

# Creating a data-frame with observed and predicted values from the model
fitted_null <- linfit_null1 %>%
  predict(train_data) %>%
  bind_cols(train_data)

# The dose data frame is Model 1
fitted_dose$label <- rep("Model 1")

#The all data frame is Model 2
fitted_all$label <- rep("Model 2")

#The null data frame is Null Model
fitted_null$label <- rep("Null Model")

#Combined data frame
fits_combined <- rbind(fitted_dose, fitted_all, fitted_null)
```

The plots differentiate each of the models by the label with color and shape for the data points. The graph plots the observed value vs the predicted value. 

When observing the graphs we created in plots 1 and plots 2, it becomes obvious that the model with all of the predictors show the greatest predictions. The Dose model and the Null model both show straight lines, which is easily explained by the properties of a null model.

```{r}
#| warning: false

# Create the ggplot figure to graph the predictive values vs the observed value for the three models
plot1 <- ggplot(
  fits_combined, aes(x = Y, y = .pred, color = label, shape = label)) +
  geom_point(size=2) +
   scale_color_manual(values = c("black", "orange", "red"))+
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +  # Adding a 45-degree line
  labs(x = "Observed Values", y = "Predicted Values", color = "Model", shape = "Model") +
  xlim(0, 5000) + 
  ylim(0, 5000)+
  theme_bw()


plot1
```

Model 2 follows the 45 degree angle line most closest compared to the other two models. 

```{r}
#| warning: false

# Create the same ggplot figure with facets
plot2 <- ggplot(
  fits_combined, aes(x = Y, y = .pred, color = label)) +
  geom_point() +
  scale_color_manual(values = c("black", "orange", "red"))+
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +  # Adding a 45-degree line
  labs(x = "Observed Values", y = "Predicted Values", color = "Model", shape = "Model") +
  theme_minimal() +
  facet_wrap(~ label, scales = "free")+ 
  xlim(0, 5000) + ylim(0, 5000)

plot2
```

The plot clearly shows a greater number of negative values compared to positive values, showing that there may be other factors still playing a role in affecting the data.

```{r}
fitted_all <- fitted_all %>%
  mutate(residuals = .pred - Y)

plot3 <- ggplot(fitted_all, aes(x = .pred, y = residuals)) +
  geom_point(size = 2, color = "purple") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +  
  labs(x = "Predicted Values", y = "Residuals") +  
  ylim(-2500, 2500) +
  theme_bw()

plot3
```


## Model Uncertainty

I start by setting a random seed and completing the bootstrap function for 100 samples.
```{r}
# Set the random seed
set.seed(443)  

# Load required libraries
#library(rsample)
#library(glmnet)

n_bs <- 100

# Create 100 bootstrap samples using training data
train_bs <- bootstraps(train_data, times = n_bs)
```

I then create a loop to fit the model and make predictions using the 100 samples. 
```{r}
# Function to make predictions using a fitted model for a given data set
make_predictions <- function(model, data) {
  predictions <- predict(model, data)
  return(predictions)
}

# Loop over each bootstrap sample, fit model, and make predictions
bs_predictions <- vector("list", length = n_bs)
for (i in 1:n_bs) {
  # Get a single bootstrap sample
  bs_sample <- rsample::analysis(train_bs$splits[[i]])
  # Make predictions using the fitted model
  bs_predictions[[i]] <- make_predictions(linfit_all1, bs_sample)
}
# Convert bs_predictions into a matrix
bs_predictions_matrix <- do.call(cbind, bs_predictions)
# Compute quantiles
preds <- apply(bs_predictions_matrix, 2, quantile, c(0.055, 0.5, 0.945))
```

I then create a data frame to plot all necessary components of the graph including observed data, point estimates from the original predicted data, median and both confidence interval bounds for each sample prediction. 

```{r}
#| warning: false
# Create a data frame containing the observed values and predictions
bs_plot <- data.frame(
  Observed = fitted_all$Y,
  PointEstimate = fitted_all$.pred,  
  Median = preds[, 2],  
  LowerBound = preds[, 1],  
  UpperBound = preds[, 3]   
)

plot4 <- ggplot(bs_plot, aes(x = Observed, y = PointEstimate)) +
  geom_point(color = "orange") +  
  geom_errorbar(aes(ymin = LowerBound, ymax = UpperBound), width = 0.1, color = "lightblue") +   
  geom_point(aes(y = Median), color = "darkred") +  
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "orange") +  
  labs(x = "Observed Values", y = "Predicted Values", title = "Observed vs. Predicted Values for Model 2") +
  xlim(0, 6000) + ylim(0, 6000)+
  theme_bw()  

# Print the plot
print(plot4)
```


Upon review of the graph, numerous overlaps are apparent between the prediction medians from the bootstrap samples and the observed values obtained from the original fit. Scattered points suggest the potential influence of additional factors on the data. The new predicted medians and the original observations closely follow the 45-degree line, indicating considerable similarity between the predicted and observed values from both the original fit and the bootstrap fits. This alignment shows the strength of the model in capturing the underlying patterns in the data. 


# This section added by Malika Dhakhwa

## Final Evaluation Using Test Data

Finally, I assessed the performance of the fit of the model including all the predictors on the test data.

```{r}
#predicting the test data
linfit_test<-lin_mod%>%fit(Y~., test_data)
fitted.all_test <- predict(linfit_test, new_data = test_data)

#print the results
tidy(linfit_test)

```

I created a plot of predicted values by observed data for both training and test data. 

First, I created a combined data frame of the values in long format.

```{r}
#| warning: false

#Creating an object for the outcome variable Y from the training data

observed_values_train <- train_data$Y

#Creating an object for the outcome variable Y from the test data

observed_values_test <- test_data$Y

#Creating a new data frame containing columns for observed and predicted values of the TRAIN data using fit of model the including all predictors. 

df_all_train <- data.frame(Observed= observed_values_train, Predicted = fitted_all$.pred, Model="Train")

#Creating a new data frame containing columns for observed and predicted values of the TEST data  using fit of the model including all predictors. 
df_all_test<- data.frame(Observed = observed_values_test, Predicted = fitted.all_test$.pred, Model="Test")

#Combining all the Observed and predicted values of the train data and test data by rows to create a long format data

combined_df <- rbind(df_all_train, df_all_test)

#Plotting of combined predicted vs observed data for the train data and test data

ggplot(combined_df, aes(x=Observed, y=Predicted, color=Model))+
  geom_point()+ 
  scale_color_manual(values = c("Train"="blue", "Test"="red")) + 
  geom_abline(intercept = 0, slope = 1, linetype = "solid", color="black") + #45 degree line
  xlim(0, 5000) + #X-axis limits
  ylim(0, 5000) + #y-axis limits
  labs(x= "Observed Values", y ="Predicted Values", title = "Predicted vs. Observed Values")+
  theme_minimal() # Use a minimal theme
```
The predictions for the test data appear to integrate well with those from the training data throughout the plot. Both sets of predictions seem to reasonably spreading around the line of perfect fit though the scatter still exhibits some pattern. Notably, in reasons where the training data predictions exhibit a wider spread, those are followed by predictions from the test data. This suggests that the model is performing well on the unseen data. While the overall spread in the predictions suggests the presence of additional factors influencing the outcome variable, the model is demonstrating a good performance when it comes to handling new data.    

From the analyses, we can say that the null model is not much informative except for predicting the mean of the outcome variable. The model predicted by Dose accounts for some variance in the outcome variable and offers a few more insights over the null model. It may be usable in studies where the effects of dose is of paramount importance than any other factors. The full model with all the predictors improve the results. However, without sufficient knowledge about the specific study, it is hard to say whether the results make sense or not or if the model can be used for any real purpose. 


=======
---
title: "fitting-exercise"
---

This exercise uses the same data as in the exercise 8 with advanced linear modelling techniques. As with exercise 8, a model is fitted to a data set (mavoglurant_A2121_nmpk) from the R package nlmixr2data. First, required packages are installed and loaded.

```{r}
#| message: false
#| warning: false
library(readr) #for loading Excel files
library(dplyr) #for data processing/cleaning
library(tidyr) #for data processing/cleaning
library(skimr) #for nice visualization of data 
library(here) #to set paths
library(ggplot2) # for plots
library(gtsummary)# for summary tables
library(patchwork) #for combine plots
library(tidymodels)
```

Setting a random seed during analysis step

```{r}
rngseed = 1234
```

Data is loaded

```{r}
#| message: false
#| warning: false
# path to data using here function
rawdata <- readr::read_csv("Mavoglurant_A2121_nmpk.csv")
```
Checking the data

```{r}
#summary(rawdata)
skimr::skim(rawdata)
```

Next, three plots of DV over time stratified by the three Dose levels are created to have a visual inspection of the data. These  plots are re-created by grouping the observations by ID following Dr. Handel's codes in solution to module 8 Exercise.

```{r}
#exploring data  

p1<- rawdata %>% ggplot()+
      geom_line(aes(x= TIME, y=DV, group = as.factor(ID), color= as.factor(DOSE))) +
      facet_wrap(~DOSE, scales = "free_y")
p1

```

The lines are not smooth. As informed in the topic, the reason for this could be that some individuals potentially have received the drug more than once, which is indicated by having both entries with OCC=1 and OCC=2. However, in absence of information on the OCC values, the analysis looks into the data set with OCC=1 only. A subset data frame is created keeping only those observations for which OCC has a value of '1'

```{r}

#Creating a sub set of data including observations with OCC=1
dataclean_step1 <- rawdata %>% filter(OCC==1)
#skimr::skim(dataclean_step1)

```
Again, a plot of DV over time stratified by Dose is created.

```{r}
#p1.1<- dataclean_step1 %>% ggplot()+
 #     geom_line(aes(x= TIME, y=DV, group = as.factor(ID), color= as.factor(DOSE))) +
  #    facet_wrap(~DOSE, scales = "free_y")
#p1.1

  
```

Next, two subsets data frame are created. In the first subset, observations wherein TIME=0 are dropped followed by computation of a variable Y which sums up the DV values for each individual based on ID. This resulted into a data frame of size 120 x 2 containing columns for ID and Y. In the second subset, only those observations are kept for which TIME=0. This second sub set has a size of 120 x 17.  Finally these two data subsets are joined to create a new data frame of size 120 x 18.  

```{r}
#Creating a subset excluding the observations with TIME=0

datasubset_1 <- dataclean_step1 %>% filter(TIME != 0) %>%
  #Grouping by ID
   group_by(ID) %>%
  #Creating variable Y, which is sum of the DV variables by ID
      dplyr::summarize(Y=sum(DV))


#Creating a subset including only observations with TIME=0 
datasubset_2 <- dataclean_step1 %>% 
                    filter(TIME==0)

#Inner joining the two subsets to create a new data frame 
data_joined <- left_join(datasubset_1, datasubset_2, by = "ID")
#skimr::skim(data_joined)

```

A final data set is created by keeping only the variables Y, DOSE, AGE, SEX, WT and HT. SEX is coded as factor. 

RACE is not included in the data. The original summary shows that RACE has 4 levels. The level 88 of RACE has 8 observations and 7 has 2 observations. In absence of the codebook, it is unclear the reason of naming those levels with numerical values distant from the other two names with top counts. 

```{r}
#sub set of data_joined keeping only the required variables
data_final <- data_joined %>%
  select(Y, DOSE, AGE, SEX, WT, HT) %>%
  #converting SEX as factors
  mutate(across(c(SEX), as.factor))

readr::write_rds(data_final, "mavoglurant.rds")

skimr::skim(data_final)
```
**Exploratory Data Analysis**

A table is created for descriptive analysis by Dose.

```{r}
# Creating a table
table1 <- tbl_summary(
  data_final,
  by = DOSE, # Stratify summary by DOSE
  type = list(
    DOSE ~ "categorical", #Specifying DOSE as categorical
    SEX ~ "categorical",  #Specifying SES as categorical
    Y ~ "continuous2",
    AGE ~ "continuous2",
    WT ~ "continuous2",
    HT ~ "continuous2"
  ),
  statistic = list(
    all_continuous() ~ c("{mean} ({sd})", "{min}, {max}"), # Statistics for continuous variables
    all_categorical() ~ "{n} ({p}%)"), # Statistics for categorical variables
  missing = "no" # Option to exclude missing data in summary
)

# Display the table
table1

```

A visual inspection of distribution of continuous variables is performed by plotting histograms. 

```{r}
#| message: false
# Plotting histograms for all continuous variables in a grid

plot1 <-
  data_final %>%
  select(Y, AGE, WT, HT) %>%
  pivot_longer(everything()) %>%
  mutate(name = factor(name, levels = c("Y", "AGE", "WT", "HT"))) %>%  #Keeps the order of plot
  ggplot(aes(x = value, fill = name)) +
  geom_histogram(alpha = 0.5, color="black") +
  facet_wrap(~name, scales = "free") +
  scale_fill_manual(values = c("Y" = "lightgreen", "AGE" = "dodgerblue", "WT" = "grey", "HT" = "salmon")) +
  theme_minimal()
plot1

#Saving the figure in the folder
plot1_file <- here("fitting-exercise", "hist_plots.png")
ggsave(filename = plot1_file, plot=plot1, bg="white")

```

Bar charts are plotted for visual inspection of distribution of factor variables. The bar chart indicates smaller representation of Dose 37.5 and Sex level of 2. 

```{r}
#creating a combined plot for DOSE, SEX and RACE. For this plot DOSE is treated as a factor as it has only 3 levels.
plot2 <- 
  data_final %>%
  mutate(DOSE = as.factor(DOSE)) %>% # Convert DOSE to factor here
  select(DOSE, SEX) %>%
  pivot_longer(everything(), names_to = "name", values_to = "value") %>%
  mutate(name = factor(name, levels = c("DOSE", "SEX"))) %>%
  ggplot(aes(x = value, fill = name)) +
  geom_bar(alpha = 0.5, color = "black") +
  facet_wrap(~name, scales = "free") +
  scale_fill_manual(values = c("DOSE" = "lightgreen", "SEX" = "salmon"))
plot2

#Saving the figure in the folder
plot2_file <- here("fitting-exercise", "factor_vars_plots.png")
ggsave(filename = plot2_file, plot=plot2, bg="white")

```

Box plot for distribution of Y by Dose is created.The box plot suggests outlier values of Y for doses 25 and 50. 

```{r}
plot3 <- data_final %>%
  mutate(DOSE = as.factor(DOSE)) %>% # Converting DOSE to factor only for the plot
  ggplot(aes(x = DOSE, y = Y, fill = DOSE)) + 
  geom_boxplot() + 
  labs(title = "Distribution of Y by DOSE", x = "DOSE", y = "Y") +
  theme_minimal()

plot3

#Saving the figure in the folder
plot3_file <- here("fitting-exercise", "Y_Dose.png")
ggsave(filename = plot3_file, plot=plot3, bg="white")
```

Next, Scatter plot is created for Y by Age stratified by Dose. For the dose of 50, the plot indicates a linear increase in Y as Age increases. For the dose of 25, such relationship is negative and for 37.50, the regression line appears comparatively stable.   

```{r}
#| message: false
plot4 <- data_final %>%
  mutate(DOSE = as.factor(DOSE)) %>% # Convert DOSE to factor here
  ggplot(aes(x = AGE, y = Y, group = DOSE, col = DOSE)) +
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + # Add a linear regression line
  scale_color_viridis_d(option = "plasma", end = .7)+
  labs(title = "Scatterplot of Y vs. AGE", x = "Age", y = "Total Drug (Y)") +
  theme_minimal()

plot4

#Saving the figure in the folder
plot4_file <- here("fitting-exercise", "Y_Age_Dose.png")
ggsave(filename = plot4_file, plot=plot4, bg="white")

```

Scatter plot is created for Y by HT stratified by Dose. For all the doses the plot indicates a decline in Y as HT increases.

```{r}
#| message: false
plot5 <- data_final %>%
  mutate(DOSE = as.factor(DOSE)) %>% # Convert DOSE to factor here
  ggplot(aes(x = HT, y = Y, group = DOSE, col = DOSE)) +
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + # Add a linear regression line
  scale_color_viridis_d(option = "plasma", end = .7)+
  labs(title = "Scatterplot of Y vs. HT", x = "HT", y = "Total Drug (Y)") +
  theme_minimal()

plot5

#Saving the figure in the folder
plot5_file <- here("fitting-exercise", "Y_HT_Dose.png")
ggsave(filename = plot5_file, plot=plot5, bg="white")

```

Scatter plot is created for Y by WT stratified by Dose. As with Y and HT, a decline in Y is observed as WT decreased for all levels of Dose. 

```{r}
#| message: false
plot6<-data_final %>%
  mutate(DOSE = as.factor(DOSE)) %>% # Convert DOSE to factor here
  ggplot(aes(x = WT, y = Y, group = DOSE, col = DOSE)) +
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + # Add a linear regression line
  scale_color_viridis_d(option = "plasma", end = .7)+
  labs(title = "Scatterplot of Y vs. WT", x = "WT", y = "Total Drug (Y)") +
  theme_minimal()

plot6

#Saving the figure in the folder
plot6_file <- here("fitting-exercise", "Y_WT_Dose.png")
ggsave(filename = plot6_file, plot=plot6, bg="white")

```

Visual inspection of correlation between the continuous variables is performed. The scatterplot matrix indicates a linear correlation between WT and HT.
```{r}
# Using the pairs function for selected variables
plot7 <- pairs(data_final[, c("Y", "AGE", "WT", "HT")], 
      main = "Pairwise Scatterplot Matrix")

plot7

#Saving the figure in the folder
plot7_file <- here("fitting-exercise", "Pair_matrix.png")
ggsave(filename = plot7_file, plot=plot6, bg="white")
```
 

**MODEL FITTING**

The data is randomly splitted into 75% train and 25% test sets following the [Data Splitting sectoin of the get Started tidymodels tutorial](https://www.tidymodels.org/start/recipes/#data-split).Linear model is fitted on the training data. In later part of the exercise the model performance will be measured by applying it on the testing data.

```{r}
#setting the random seed for reproducibility
set.seed(rngseed)

#Assigning 75% of the data into the training set
data_split <- initial_split(data_final, prop = .75)

#Creating data frames for the train and test data
train_data <- training(data_split)
test_data <- testing(data_split)

#checking the data structure
str(train_data)
str(test_data)

```

**Linear Model**

Two linear models are fitted to the continuous outcome (Y) of the train data, first using only the main predictor of interest DOSE and second  using all predictors. A null model is also fitted.

```{r}
#For reproducibility
set.seed(rngseed)

#Using linear regression function from tidymodels. The default for linear_reg() is "lm" for OLS
lin_mod <- linear_reg() %>% set_engine("lm") 

#fitting Y on Dose
linfit_dose1 <- lin_mod%>%fit(Y ~ DOSE, data = train_data)

#Fitting Y on all other variables in the data
linfit_all1 <- lin_mod%>%fit(Y~., data=train_data)

# Fitting a null model using tidymodels' parsnip engine
null_mod <- null_model() %>% set_engine("parsnip") %>% set_mode("regression")
linfit_null1 <- null_mod %>%
  fit(Y ~ 1, data = train_data)

#Using tidy for better format of the result table
tidy(linfit_dose1)
tidy(linfit_all1)
tidy(linfit_null1)

```

The results of both the first and second models suggest a positive relationship between the Y and DOSE. The second model suggests that controlling for the effects of other variables, there is a positive relationship of Y with AGE and a negative relation with each of WT and HT. Holding all other variables constant, Y is expected to decrease for a change in SEX variable from 1 to 2.   

Next, RMSE metrics are computed for all the models following Dr. Handel's codes for solutions to module 8 exercise.

```{r}
#| message: false
#| warning: false
#Computing the RMSE for model 1
metrics_dose1 <- linfit_dose1 %>%
  predict(train_data) %>%
  bind_cols(train_data)%>%
  metrics(truth=Y, estimate=.pred)

#Computing the RMSE for model 2
metrics_all1 <- linfit_all1 %>%
  predict(train_data) %>%
  bind_cols(train_data)%>%
  metrics(truth=Y, estimate=.pred)

#Computing the RMSE for model 3
metrics_null1 <- linfit_null1 %>%
  predict(train_data) %>%
  bind_cols(train_data)%>%
  metrics(truth=Y, estimate=.pred)


#print the results
print(metrics_null1)
print(metrics_dose1)
print(metrics_all1)

```

The RMSEs are 948, 702 and 627 for the null model, model with only DOSE and model with all the predictors respectively. RMSE measures the average difference between the predicted values from the model and the actual values in the data. A lower RMSE value indicates a better fit to the data. Thus, the linear model containing all predictor variables outperforms the other two models according to the RMSE metrics.

Subsequently, the model performance is computed using cross-validation technique with 10-folds for the two models. This CV technique subsamples the training data 10 times and fits each of the models to the data, wherein 90% of the data is used to fit the model, and 10% to evaluate the model. 

```{r}
#setting the random seed for reproducibility
set.seed(rngseed)

folds <-vfold_cv(train_data, v=10)
folds
```

In the following steps, an object is built for resampling using 'workflow' function of tidymodels. The 'workflow' function bundles together the pre-processing, modeling, and post-processing requests.

```{r}
#setting the random seed for reproducibility
set.seed(rngseed)
#Resampling using workflow for the model with only DOSE as predictor
linfit_dose2 <- 
	workflow() %>%
	add_model(lin_mod) %>%
  add_formula(Y ~ DOSE)%>%
	fit_resamples(folds)

#Resampling using workflowfor for the model with all predictors
linfit_all2 <- 
	workflow() %>%
	add_model(lin_mod) %>%
  add_formula(Y ~ .)%>%
	fit_resamples(folds)

#extracting the performance statistics results created from the 10 assessment sets. 

collect_metrics(linfit_dose2)
collect_metrics(linfit_all2)

```

The application of 10-fold CV led to an improvement in the RMSE of the model including only DOSE, reducing it to 690 from 702, which was observed when the training data was utilized without CV. Conversely, the RMSE of the model including all the predictors experienced an increase to 645 with 10-fold CV, compared to 627 without CV. 

Unlike the train/test model without CV, the 10-fold CV calculates 10 separate RMSE values for each sample and averages these values. This process introduces some variability among the generated RMSEs. Analysis revealed that the full model exhibits a smaller standard error of 64.81 for RMSE compared to 67.49 of the model that includes only the DOSE predictor.


Finally, the 10-fold CV modelling is performed again to check how different the metric is when using a different seed for randomization. 

```{r}
#setting a different random seed
set.seed(222)

#Assigning 75% of the data into the training set
data_split_new <- initial_split(data_final, prop = .75)

#Creating data frames for the train and test data
train_data_new <- training(data_split_new)
test_data_new <- testing(data_split_new)

```

Preparing the data for 10-fold cross-validation.
```{r}
#setting a different random seed
set.seed(222)

#Creating 10 random samples of the newly generated training data
folds_new <-vfold_cv(train_data_new, v=10)

```

Using the workflow to compute both the models.
```{r}
#setting the random seed for reproducibility
set.seed(222)

#Resampling using workflow for the model with only DOSE as predictor
linfit_dose2_new <- 
	workflow() %>%
	add_model(lin_mod) %>%
  add_formula(Y ~ DOSE)%>%
	fit_resamples(folds_new)

#Resampling using workflowfor for the model with all predictors
linfit_all2_new <- 
	workflow() %>%
	add_model(lin_mod) %>%
  add_formula(Y ~ .)%>%
	fit_resamples(folds_new)

#extracting the performance statistics results created from the 10 assessment sets. 

collect_metrics(linfit_dose2_new)
collect_metrics(linfit_all2_new)

```
 
When a new seed is applied, further reduction is noted in the mean RMSE for the model that included only DOSE as predictor, with value dropping to 664 from the previous seed result of 690.The Standard Error remained relatively unchanged from the previous figure. For the full model, the mean RMSE saw a decrease to 634 with the new seed, down from 648 with the the previous seed. Despite this reduction, the RMSE of the full model still remained higher than that of the full model using the training data without CV. 

In summary, the model that included only DOSE, when subjected to 10-fold CV, showed an optimized RMSE compared to the models fitted on the training data without CV. On the other hand, for the full model, a better RMSE is observed when the model is fitted on the training data without CV.    

# This section added by MUTSA NYAMURANGA

## Model Predictions

I create 3 data frames that combine the observed and predicted values from the 3 original model fits to all of the training data. I also create labels for each model. 

```{r}
# Creating a data-frame with observed and predicted values from the model with `DOSE` as the predictor
fitted_dose <- linfit_dose1 %>%
  predict(train_data) %>%
  bind_cols(train_data)

# Creating a data-frame with observed and predicted values from the model with everything as the predictor
fitted_all <- linfit_all1 %>%
  predict(train_data) %>%
  bind_cols(train_data)

# Creating a data-frame with observed and predicted values from the model
fitted_null <- linfit_null1 %>%
  predict(train_data) %>%
  bind_cols(train_data)

# The dose data frame is Model 1
fitted_dose$label <- rep("Model 1")

#The all data frame is Model 2
fitted_all$label <- rep("Model 2")

#The null data frame is Null Model
fitted_null$label <- rep("Null Model")

#Combined data frame
fits_combined <- rbind(fitted_dose, fitted_all, fitted_null)
```

The plots differentiate each of the models by the label with color and shape for the data points. The graph plots the observed value vs the predicted value. 

When observing the graphs we created in plots 1 and plots 2, it becomes obvious that the model with all of the predictors show the greatest predictions. The Dose model and the Null model both show straight lines, which is easily explained by the properties of a null model.

```{r}
# Create the ggplot figure to graph the predictive values vs the observed value for the three models
plot1 <- ggplot(
  fits_combined, aes(x = Y, y = .pred, color = label, shape = label)) +
  geom_point(size=2) +
   scale_color_manual(values = c("black", "orange", "red"))+
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +  # Adding a 45-degree line
  labs(x = "Observed Values", y = "Predicted Values", color = "Model", shape = "Model") +
  xlim(0, 5000) + 
  ylim(0, 5000)+
  theme_bw()


plot1
```

Model 2 follows the 45 degree angle line most closest compared to the other two models. 

```{r}
# Create the same ggplot figure with facets
plot2 <- ggplot(
  fits_combined, aes(x = Y, y = .pred, color = label)) +
  geom_point() +
  scale_color_manual(values = c("black", "orange", "red"))+
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +  # Adding a 45-degree line
  labs(x = "Observed Values", y = "Predicted Values", color = "Model", shape = "Model") +
  theme_minimal() +
  facet_wrap(~ label, scales = "free")+ 
  xlim(0, 5000) + ylim(0, 5000)

plot2
```

The plot clearly shows a greater number of negative values compared to positive values, showing that there may be other factos still playing a role in affecting the data. 
```{r}
fitted_all <- fitted_all %>%
  mutate(residuals = .pred - Y)

plot3 <- ggplot(fitted_all, aes(x = .pred, y = residuals)) +
  geom_point(size = 2, color = "purple") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +  
  labs(x = "Predicted Values", y = "Residuals") +  
  ylim(-2500, 2500) +
  theme_bw()

plot3
```


## Model Uncertainty

I start by setting a random seed and completing the bootstrap function for 100 samples.
```{r}
# Set the random seed
set.seed(443)  

# Load required libraries
library(rsample)
library(glmnet)

n_bs <- 100

# Create 100 bootstrap samples using training data
train_bs <- bootstraps(train_data, times = n_bs)
```

I then create a loop to fit the model and make predictions using the 100 samples. 
```{r}
# Function to make predictions using a fitted model for a given data set
make_predictions <- function(model, data) {
  predictions <- predict(model, data)
  return(predictions)
}

# Loop over each bootstrap sample, fit model, and make predictions
bs_predictions <- vector("list", length = n_bs)
for (i in 1:n_bs) {
  # Get a single bootstrap sample
  bs_sample <- rsample::analysis(train_bs$splits[[i]])
  # Make predictions using the fitted model
  bs_predictions[[i]] <- make_predictions(linfit_all1, bs_sample)
}
# Convert bs_predictions into a matrix
bs_predictions_matrix <- do.call(cbind, bs_predictions)
# Compute quantiles
preds <- apply(bs_predictions_matrix, 2, quantile, c(0.055, 0.5, 0.945))
```

I then create a data frame to plot all necessary components of the graph including observed data, point estimates from the original predicted data, median and both confidence interval bounds for each sample prediction. 

```{r}
# Create a data frame containing the observed values and predictions
bs_plot <- data.frame(
  Observed = fitted_all$Y,
  PointEstimate = fitted_all$.pred,  
  Median = preds[, 2],  
  LowerBound = preds[, 1],  
  UpperBound = preds[, 3]   
)

plot4 <- ggplot(bs_plot, aes(x = Observed, y = PointEstimate)) +
  geom_point(color = "orange") +  
  geom_errorbar(aes(ymin = LowerBound, ymax = UpperBound), width = 0.1, color = "lightblue") +   
  geom_point(aes(y = Median), color = "darkred") +  
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "orange") +  
  labs(x = "Observed Values", y = "Predicted Values", title = "Observed vs. Predicted Values for Model 2") +
  xlim(0, 6000) + ylim(0, 6000)+
  theme_bw()  

# Print the plot
print(plot4)
```


Upon review of the graph, numerous overlaps are apparent between the prediction medians from the bootstrap samples and the observed values obtained from the original fit. Scattered points suggest the potential influence of additional factors on the data. The new predicted medians and the original observations closely follow the 45-degree line, indicating considerable similarity between the predicted and observed values from both the original fit and the bootstrap fits. This alignment shows the strength of the model in capturing the underlying patterns in the data. 















>>>>>>> cf54ac50c0f76d75e69631ade1511ba40e953bb1
